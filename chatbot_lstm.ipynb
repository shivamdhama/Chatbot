{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from keras.layers import Input, Embedding, LSTM, Dense,Bidirectional, RepeatVector, Bidirectional, Dropout, merge\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('rdany_conversations_2016-03-01.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "HIDDEN_UNITS = 512\n",
    "MAX_INPUT_SEQ_LENGTH = 40\n",
    "MAX_TARGET_SEQ_LENGTH = 40\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "WEIGHT_FILE_PATH = 'model1.h5'\n",
    "WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  \n",
    "epochs = 100  \n",
    "latent_dim = 256  \n",
    "num_samples = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_lines = open('human_text.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "robot_lines = open('robot_text.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = human_lines\n",
    "answers = robot_lines  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "        \n",
    "    clean_answers.append(clean_text(answer))\n",
    "\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2em = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word2em[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(word2em))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_counter = Counter()\n",
    "input_counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ques=[]\n",
    "for sent in clean_questions:\n",
    "    words = []\n",
    "    words = sent.split()\n",
    "    for w in words:\n",
    "            input_counter[w] += 1\n",
    "    ques.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans=[]\n",
    "for sent in clean_answers:\n",
    "    words = []\n",
    "    words = sent.split()\n",
    "    words.insert(0, 'start')\n",
    "    words.append('end')\n",
    "        \n",
    "    for w in words:\n",
    "            target_counter[w] += 1\n",
    "    ans.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_word2idx = dict()\n",
    "for idx, word in enumerate(target_counter.most_common(70000)):\n",
    "    target_word2idx[word[0]] = idx + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'unknown' not in target_word2idx:\n",
    "    target_word2idx['unknown'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_word2idx = dict()\n",
    "for idx, word in enumerate(input_counter.most_common(70000)):\n",
    "    input_word2idx[word[0]] = idx + 1\n",
    "\n",
    "if 'unknown' not in input_word2idx:\n",
    "    input_word2idx['unknown'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_decoder_tokens = len(target_idx2word)+1\n",
    "num_encoder_tokens = len(input_idx2word)+1\n",
    "\n",
    "input_texts_word2em = []\n",
    "\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "for input_words, target_words in zip(ques, ans):\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        emb = np.zeros(shape=100)\n",
    "        if w in word2em:\n",
    "            emb = word2em[w]\n",
    "        encoder_input_wids.append(emb)\n",
    "\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_decoder_tokens': 3381, 'encoder_max_seq_length': 254, 'decoder_max_seq_length': 146, 'num_encoder_tokens': 3668}\n"
     ]
    }
   ],
   "source": [
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
    "context['num_encoder_tokens'] = num_encoder_tokens\n",
    "\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(input_word2em_data, output_text_data):\n",
    "    num_batches = len(input_word2em_data) // BATCH_SIZE\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            encoder_input_data_batch = np.array(pad_sequences(input_word2em_data[start:end], encoder_max_seq_length))\n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, GLOVE_EMBEDDING_SIZE))\n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = target_word2idx['unknown']  # default unknown\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w in word2em:\n",
    "                        decoder_input_data_batch[lineIdx, idx, :] = word2em[w]\n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890\n",
      "473\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, ans, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(Xtrain))\n",
    "print(len(Xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "58/59 [============================>.] - ETA: 2s - loss: 0.4103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhama/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 151s 3s/step - loss: 0.4087 - val_loss: 0.3910\n",
      "Epoch 2/50\n",
      "59/59 [==============================] - 154s 3s/step - loss: 0.3640 - val_loss: 0.3791\n",
      "Epoch 3/50\n",
      "59/59 [==============================] - 143s 2s/step - loss: 0.3417 - val_loss: 0.3724\n",
      "Epoch 4/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.3249 - val_loss: 0.3673\n",
      "Epoch 5/50\n",
      "59/59 [==============================] - 133s 2s/step - loss: 0.3105 - val_loss: 0.3656\n",
      "Epoch 6/50\n",
      "59/59 [==============================] - 131s 2s/step - loss: 0.2975 - val_loss: 0.3620\n",
      "Epoch 7/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.2874 - val_loss: 0.3608\n",
      "Epoch 8/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.2746 - val_loss: 0.3613\n",
      "Epoch 9/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.2637 - val_loss: 0.3648\n",
      "Epoch 10/50\n",
      "59/59 [==============================] - 132s 2s/step - loss: 0.2553 - val_loss: 0.3621\n",
      "Epoch 11/50\n",
      "59/59 [==============================] - 135s 2s/step - loss: 0.2465 - val_loss: 0.3638\n",
      "Epoch 12/50\n",
      "59/59 [==============================] - 132s 2s/step - loss: 0.2326 - val_loss: 0.3649\n",
      "Epoch 13/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.2256 - val_loss: 0.3639\n",
      "Epoch 14/50\n",
      "59/59 [==============================] - 135s 2s/step - loss: 0.2119 - val_loss: 0.3651\n",
      "Epoch 15/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.2036 - val_loss: 0.3673\n",
      "Epoch 16/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.1951 - val_loss: 0.3688\n",
      "Epoch 17/50\n",
      "59/59 [==============================] - 134s 2s/step - loss: 0.1819 - val_loss: 0.3707\n",
      "Epoch 18/50\n",
      "59/59 [==============================] - 131s 2s/step - loss: 0.1719 - val_loss: 0.3714\n",
      "Epoch 19/50\n",
      "59/59 [==============================] - 131s 2s/step - loss: 0.1657 - val_loss: 0.3744\n",
      "Epoch 20/50\n",
      "59/59 [==============================] - 131s 2s/step - loss: 0.1541 - val_loss: 0.3747\n",
      "Epoch 21/50\n",
      "59/59 [==============================] - 134s 2s/step - loss: 0.1452 - val_loss: 0.3766\n",
      "Epoch 22/50\n",
      "59/59 [==============================] - 132s 2s/step - loss: 0.1365 - val_loss: 0.3804\n",
      "Epoch 23/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.1310 - val_loss: 0.3826\n",
      "Epoch 24/50\n",
      "59/59 [==============================] - 134s 2s/step - loss: 0.1216 - val_loss: 0.3879\n",
      "Epoch 25/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.1138 - val_loss: 0.3860\n",
      "Epoch 26/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.1062 - val_loss: 0.3909\n",
      "Epoch 27/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.1013 - val_loss: 0.3916\n",
      "Epoch 28/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0927 - val_loss: 0.3971\n",
      "Epoch 29/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0870 - val_loss: 0.3969\n",
      "Epoch 30/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0826 - val_loss: 0.4005\n",
      "Epoch 31/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0758 - val_loss: 0.4041\n",
      "Epoch 32/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0799 - val_loss: 0.4038\n",
      "Epoch 33/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0675 - val_loss: 0.4082\n",
      "Epoch 34/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0621 - val_loss: 0.4113\n",
      "Epoch 35/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0585 - val_loss: 0.4158\n",
      "Epoch 36/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0538 - val_loss: 0.4145\n",
      "Epoch 37/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0493 - val_loss: 0.4203\n",
      "Epoch 38/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0451 - val_loss: 0.4226\n",
      "Epoch 39/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0416 - val_loss: 0.4244\n",
      "Epoch 40/50\n",
      "59/59 [==============================] - 131s 2s/step - loss: 0.0383 - val_loss: 0.4279\n",
      "Epoch 41/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0403 - val_loss: 0.4264\n",
      "Epoch 42/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0388 - val_loss: 0.4295\n",
      "Epoch 43/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0320 - val_loss: 0.4323\n",
      "Epoch 44/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0285 - val_loss: 0.4367\n",
      "Epoch 45/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0277 - val_loss: 0.4368\n",
      "Epoch 46/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0252 - val_loss: 0.4410\n",
      "Epoch 47/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0220 - val_loss: 0.4452\n",
      "Epoch 48/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0211 - val_loss: 0.4478\n",
      "Epoch 49/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0194 - val_loss: 0.4488\n",
      "Epoch 50/50\n",
      "59/59 [==============================] - 130s 2s/step - loss: 0.0175 - val_loss: 0.4519\n"
     ]
    }
   ],
   "source": [
    "train_gen = generate_batch(Xtrain, Ytrain)\n",
    "test_gen = generate_batch(Xtest, Ytest)\n",
    "\n",
    "train_num_batches = len(Xtrain)// BATCH_SIZE \n",
    "test_num_batches = len(Xtest) // BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
    "\n",
    "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                    epochs=50,\n",
    "                    verbose=1, validation_data=test_gen,\n",
    "                    validation_steps=test_num_batches,\n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "model.save_weights(WEIGHT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def in_white_list(_word):\n",
    "    for char in _word:\n",
    "        if char in WHITELIST:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi how are you\n",
      "why are you sad 😞\n",
      "good night\n",
      "i am a robot and i am not a girl and not a boy 😄\n"
     ]
    }
   ],
   "source": [
    "def reply(input_text):\n",
    "    input_seq = []\n",
    "    input_emb = []\n",
    "    for word in input_text.lower().split():\n",
    "        if not in_white_list(word):\n",
    "            continue\n",
    "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
    "        if word in word2em:\n",
    "            emb = word2em[word]\n",
    "        input_emb.append(emb)\n",
    "    input_seq.append(input_emb)\n",
    "    input_seq = pad_sequences(input_seq,MAX_INPUT_SEQ_LENGTH)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "    target_seq[0, 0, :] = word2em['start']\n",
    "    target_text = ''\n",
    "    target_text_len = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        sample_word = target_idx2word[sample_token_idx]\n",
    "        target_text_len += 1\n",
    "\n",
    "        if sample_word != 'start' and sample_word != 'end':\n",
    "            target_text += ' ' + sample_word\n",
    "\n",
    "        if sample_word == '\\n' or target_text_len >= MAX_TARGET_SEQ_LENGTH or sample_word == 'end':\n",
    "            terminated = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "        if sample_word in word2em:\n",
    "            target_seq[0, 0, :] = word2em[sample_word]\n",
    "\n",
    "        states_value = [h, c]\n",
    "    return target_text.strip()\n",
    "\n",
    "def test_run():\n",
    "    print(reply('Hello')) \n",
    "    print(reply('I am sad'))\n",
    "    print(reply('good night'))\n",
    "    print(reply('who are you?'))\n",
    "   \n",
    "test_run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhama/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('chat.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
